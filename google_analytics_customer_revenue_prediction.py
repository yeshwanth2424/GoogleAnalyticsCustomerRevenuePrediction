# -*- coding: utf-8 -*-
"""Google Analytics Customer Revenue Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FC2GEMJz3b9hMjHTvbO9ReO3xxRvg1lg

# GOOGLE ANALYTICS CUSTOMER REVENUE PREDICTION

# 1.) Data Loading and Preprocessing
"""

!pip install lightgbm

!pip install xgboost

#import libraries

import warnings
warnings.filterwarnings("ignore")
import pandas as pd
import numpy as np
import json
import pickle
import missingno as msno
from pandas.io.json import json_normalize
import matplotlib.pyplot as plt
import os
from datetime import datetime, timedelta
from sklearn import preprocessing 
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import SGDRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error
import lightgbm as lgb 
import xgboost as xgb
from prettytable import PrettyTable

#sample data display to have basic understanding  about the structure of data

df = pd.read_csv('train_v2.csv',nrows=5)
df

#display data types

df.dtypes

# The below function is used to preprocess the data and produce proper data frames for train and test datasets

def loadAndCreate_df(filepath,n_rows=None,usefulfeatures=[]):
    
    #Json data features are listed our here
    json_features = ['device','geoNetwork','totals','trafficSource']
    
    
    #Reading data from the given filepath with chunksizes of 100k rows in each
    train_df_chunks = pd.read_csv(filepath,
                converters = {feature:json.loads for feature in json_features},
                nrows=n_rows,
                dtype = {'fullVisitorId':'str','channelGrouping':'str','visitId':'int',
                         'visitNumber':'int','visitStartTime':'int'},
                parse_dates = ['date'],
                chunksize = 100000)
    
    #Create an empty dataframe and append chunked data to it in every iteration   
    train_df = pd.DataFrame()
    
    #Append all chunks to train_df in multiple iterations
    for df_chunk in train_df_chunks:
        df_chunk.reset_index(drop=True,inplace=True)
        
        #Normalize json features  
        for feature in json_features:
            json_flattened = json_normalize(df_chunk[feature])
            json_flattened_features = []
            
            for each_col in json_flattened.columns:
                json_flattened_features.append(feature+"."+each_col)
                
            json_flattened.columns = json_flattened_features
            
            #drop the existing json data and append the normalized data
            df_chunk = df_chunk.drop(feature,axis=1).merge(json_flattened,left_index=True,right_index=True)
        
        #By default all features are selected if no feature is sent as input
        if len(usefulfeatures)==0:
            usefulfeatures = df_chunk.columns
            
        #select the useful features into useful data frame
        useful_df = df_chunk[usefulfeatures]
        train_df = pd.concat([train_df,useful_df],axis=0).reset_index(drop=True)
        print(train_df.shape)
        del df_chunk
    return train_df

# Commented out IPython magic to ensure Python compatibility.
# select the first 100000 rows for data cleaning purpose
# %time train_df = loadAndCreate_df('train_v2.csv',n_rows=100000,usefulfeatures=[])

#display existing columns before data cleaning
train_df.columns

"""# 2.) Data Cleaning"""

#Data cleaning includes the exclusion of constant and redundant columns data
constant_columns = []
for each_col in train_df.columns:
    if train_df[each_col].nunique()==1:
        constant_columns.append(each_col)

#custom dimensions and hits contains non-useful data
useless_features = constant_columns + ['customDimensions','hits']

#display useful features data after data cleaning
usefulfeatures = list(set(train_df.columns)-set(useless_features))

# Commented out IPython magic to ensure Python compatibility.
#Load entire train data after data cleaning
# %time train_df = loadAndCreate_df('train_v2.csv',usefulfeatures=usefulfeatures)

# Commented out IPython magic to ensure Python compatibility.
#Load entire test data after data cleaning
# %time test_df = loadAndCreate_df('test_v2.csv',usefulfeatures=usefulfeatures)

"""### Plot missing values plot to eliminate useless features"""

msno.bar(train_df)

#conacat few other useless features
useless_features = useless_features + ['trafficSource.campaign','trafficSource.adContent','trafficSource.adwordsClickInfo.gclId',
                                      'trafficSource.adwordsClickInfo.page','trafficSource.adwordsClickInfo.slot',
                                       'trafficSource.adwordsClickInfo.adNetworkType','visitId']

#display useful features data after data cleaning
usefulfeatures = list(set(train_df.columns)-set(useless_features))
usefulfeatures

#drop few other useless features from train dataframe
train_df=train_df.drop(['trafficSource.campaign','trafficSource.adContent','trafficSource.adwordsClickInfo.gclId',
                                      'trafficSource.adwordsClickInfo.page','trafficSource.adwordsClickInfo.slot',
                                       'trafficSource.adwordsClickInfo.adNetworkType','visitId'], axis=1)

#drop few other useless features from test dataframe
test_df=test_df.drop(['trafficSource.campaign','trafficSource.adContent','trafficSource.adwordsClickInfo.gclId',
                                      'trafficSource.adwordsClickInfo.page','trafficSource.adwordsClickInfo.slot',
                                       'trafficSource.adwordsClickInfo.adNetworkType','visitId'], axis=1)

#print train and test shapes
print(train_df.shape)
print(test_df.shape)

#save train and test dataframes into pickle file
train_df.to_pickle('preprocessed_train_df')
test_df.to_pickle('preprocessed_test_df')

#load train and test dataframes
train_df = pd.read_pickle('preprocessed_train_df')
test_df = pd.read_pickle('preprocessed_test_df')

#save train and test dataframes as csv files
train_df.to_csv('preprocessed_train_df.csv')
test_df.to_csv('preprocessed_test_df.csv')

#columns in train data frame
train_df.columns

#columns in test data frame
test_df.columns

"""# 3.) Exploratory Data Analysis"""

#Font dictionary is defined for EDA plots

font = {'color':'black','weight':'bold','size':20}
tick_color = '#104E8B'

"""## i.)Transaction Revenue Analysis"""

train_df["totals.transactionRevenue"] = train_df["totals.transactionRevenue"].astype('float')

revenuePerVisitor = train_df.groupby("fullVisitorId")["totals.transactionRevenue"].sum().reset_index() 

plt.figure(figsize=(8,6))
plt.scatter(range(len(revenuePerVisitor)), np.sort(np.log1p(revenuePerVisitor["totals.transactionRevenue"].values)))
plt.xlabel('Visitor-Index',fontdict=font)
plt.ylabel('Transaction Revenue Per Visitor',fontdict=font)
plt.xticks(fontsize=12, color=tick_color)
plt.yticks(color=tick_color)
plt.title('Transaction revenue of All visitors',fontdict=font)
plt.legend(['Transaction Revenue'])
plt.show()

revenue_list = np.array(revenuePerVisitor["totals.transactionRevenue"])
reveneue_customer_percent = round(((len(revenue_list[np.where(revenue_list!=0)])/len(revenue_list))*100),2)
print("The percentage of visitors from revenue generated is "+ str(reveneue_customer_percent)+"%")

"""## Observations
- The 80/20 rule in Businesses has proven that most of the revenue will be generated by very few customers.
- So,the percentage of visitors from which revenue generated is 1.21%

## ii.) Channel grouping Analysis
"""

#plot figure
plt.figure(figsize=(15,5))

# subplot-1
cgp_count = train_df.groupby("channelGrouping")["fullVisitorId"].count().reset_index().sort_values(["fullVisitorId"],ascending=False).reset_index(drop=True)
cgp_count['fullVisitorId'] = cgp_count['fullVisitorId']/1000000

x = cgp_count["channelGrouping"]
y = cgp_count["fullVisitorId"]

plt.subplot(1,2,1)
plt.bar(x,y)
plt.xlabel('channel',fontdict=font)
plt.ylabel('No. of visitors in millions',fontdict=font)
plt.xticks(rotation=90,fontsize=12, color=tick_color)
plt.yticks(color=tick_color)
plt.title("Visitor Count per channel",fontdict=font)
plt.legend(['visitor count'])

# subplot-2
cgp_revenue = train_df.groupby("channelGrouping")["totals.transactionRevenue"].sum().reset_index().sort_values(["totals.transactionRevenue"],ascending=False).reset_index(drop=True)
cgp_revenue['totals.transactionRevenue'] = np.log1p(cgp_revenue["totals.transactionRevenue"].values)

x = cgp_revenue["channelGrouping"]
y = cgp_revenue["totals.transactionRevenue"]

plt.subplot(1,2,2)
plt.bar(x,y)
plt.xlabel('channel',fontdict=font)
plt.ylabel('Log Transaction Revenue',fontdict=font)
plt.xticks(rotation=90,fontsize=12, color=tick_color)
plt.yticks(color=tick_color)
plt.title("Log Transaction Revenue per channel",fontdict=font)
plt.legend(['Log Transaction Revenue'])
plt.show()

"""## Observations
- From the above plot we can see that most of the revenue is generated through Referral,Direct, organic search
- Even though the number of visits are less through referral and direct the revneue generated is huge.So, Analytics team can invest less money for direct and referral and can generate high revenue.
- If we can observe the revenue generated through Referral is huge because any purchase of the product through referral codes or links will have an offer on the product which makes customers to purchase that product with high probability.

## iii.) Web Browsing Analysis
"""

#plot figure
plt.figure(figsize=(15,5))

# subplot-1
browser_count = train_df.groupby("device.browser")["fullVisitorId"].count().reset_index().sort_values(["fullVisitorId"],ascending=False).reset_index(drop=True)
x_temp = np.array(browser_count["device.browser"])
y_temp = np.array(browser_count["fullVisitorId"])

#filtering web browser visitor count with atleast 100 count for finding huge revenue happened through particular web browser
index=np.where(y_temp>100)
x=x_temp[index]
y=y_temp[index]

#In bar plot visitor count scale will be in millions
y=y/1000000

plt.subplot(1,2,1)
plt.bar(x,y)
plt.xlabel('Browser',fontdict=font)
plt.ylabel('No. of visitors in millions',fontdict=font)
plt.xticks(rotation=90,fontsize=10,color=tick_color)
plt.yticks(color=tick_color)
plt.title("Visitor Count per web browser",fontdict=font)
plt.legend(['visitor count'])

# subplot-2
browser_revenue = train_df.groupby("device.browser")["totals.transactionRevenue"].sum().reset_index().sort_values(["totals.transactionRevenue"],ascending=False).reset_index(drop=True)
browser_revenue['totals.transactionRevenue'] = np.log1p(browser_revenue["totals.transactionRevenue"].values)

x_temp = np.array(browser_revenue["device.browser"])
y_temp = np.array(browser_revenue["totals.transactionRevenue"])

x=x_temp[index]
y=y_temp[index]


plt.subplot(1,2,2)
plt.bar(x,y)
plt.xlabel('Browser',fontdict=font)
plt.ylabel('Log Transaction Revenue',fontdict=font)
plt.xticks(rotation=90,fontsize=10,color=tick_color)
plt.yticks(color=tick_color)
plt.title("Log Transaction Revenue per Browser",fontdict=font)
plt.legend(['Log Transaction Revenue'])
plt.show()

"""## Observations
- From the above plot we can see that huge revenue is generated through chrome, firefox, safari etc
- Important observation is that even after having less number of visits through safari, firefox the revenue generated through these browsers are on par with revneue generated through chrome browser.So, Analytics team can invest less money on safari, firefox and can generate huge revenue.

## iv.) Operating system Analysis
"""

#plot figure
plt.figure(figsize=(16,5))

# subplot-1
os_count = train_df.groupby("device.operatingSystem")["fullVisitorId"].count().reset_index().sort_values(["fullVisitorId"],ascending=False).reset_index(drop=True)
x = os_count["device.operatingSystem"]
y = os_count["fullVisitorId"]

#In bar plot visitor count scale will be in millions
y=y/1000000

plt.subplot(1,2,1)
plt.bar(x,y)
plt.xlabel('Operating system',fontdict = font)
plt.ylabel('No. of visitors in millions',fontsize=20,fontdict = font)
plt.xticks(rotation=90,fontsize=12, color=tick_color)
plt.yticks(color=tick_color)
plt.title("Visitor Count per operating system ",fontdict=font)
plt.legend(['visitor count'])

# subplot-2
os_revenue = train_df.groupby("device.operatingSystem")["totals.transactionRevenue"].sum().reset_index().sort_values(["totals.transactionRevenue"],ascending=False).reset_index(drop=True)
os_revenue['totals.transactionRevenue'] = np.log1p(os_revenue["totals.transactionRevenue"].values)

x = os_revenue["device.operatingSystem"]
y = os_revenue["totals.transactionRevenue"]

plt.subplot(1,2,2)
plt.bar(x,y)
plt.xlabel('Operating system',fontdict=font)
plt.ylabel('Log Transaction Revenue',fontdict=font)
plt.xticks(rotation=90,fontsize=12,color=tick_color)
plt.yticks(color=tick_color)
plt.title("Log Transaction Revenue per operating system",fontdict=font)
plt.legend(['Log Transaction Revenue'])
plt.show()

"""## Observations
- From the above plot we can see that most of the revneue is generated through Macintosh,windows,chromeOS,Linux,Android,ios
- Important observation from the above plot is even though we have less number of visits in linux, android, ios the revenue generated through Android,ios are on par with macintosh and windows.
- The revenue generated through Android,ios,windows phone is huge even though there are less number of visits because one the major reason could be as these operating systems are mostly present in mobile devices. So, most number of purchases of costly products could have been done through mobile devices with these operating systems.

## v.) Device category Analysis
"""

#plot figure
plt.figure(figsize=(15,5))

# subplot-1
device_count = train_df.groupby("device.deviceCategory")["fullVisitorId"].count().reset_index().sort_values(["fullVisitorId"],ascending=False).reset_index(drop=True)
x = device_count["device.deviceCategory"]
y = device_count["fullVisitorId"]

#In bar plot visitor count scale will be in millions
y=y/1000000

plt.subplot(1,2,1)
plt.bar(x,y,width=0.4)
plt.xlabel('Device',fontdict=font)
plt.ylabel('No. of visitors in millions',fontdict=font)
plt.xticks(fontsize=14, color=tick_color)
plt.yticks(color=tick_color)
plt.title("Visitor Count per device ",fontdict=font)
plt.legend(['visitor count'])

# subplot-2
device_revenue = train_df.groupby("device.deviceCategory")["totals.transactionRevenue"].sum().reset_index().sort_values(["totals.transactionRevenue"],ascending=False).reset_index(drop=True)
device_revenue['totals.transactionRevenue'] = np.log1p(device_revenue["totals.transactionRevenue"].values)

x = device_revenue["device.deviceCategory"]
y = device_revenue["totals.transactionRevenue"]

plt.subplot(1,2,2)
plt.bar(x,y,width=0.4)
plt.xlabel('Device',fontdict=font)
plt.ylabel('Log Transaction Revenue',fontdict=font)
plt.xticks(fontsize=14, color=tick_color)
plt.yticks(color=tick_color)
plt.title("Log Transaction Revenue per device",fontdict=font)
plt.legend(['Log Transaction Revenue'])
plt.show()

"""## Observations
- From the above plot we can observe that the number of visits and generated revenue both are high through desktop
- Even though the number of visits on mobile device and tablet very less when compared to desktop the revenue generated through them are very high
- Especially the number of visits through tablet are very less but the revenue generated through it is huge.

## vi.) Mobile vs Non-Mobile Analysis
"""

#plot figure
plt.figure(figsize=(15,5))

# subplot-1
mobile_count = train_df.groupby("device.isMobile")["fullVisitorId"].count().reset_index().sort_values(["fullVisitorId"],ascending=False).reset_index(drop=True)

x = mobile_count["device.isMobile"]
x=[str(x[i]) for i in range(len(x))]
y = mobile_count["fullVisitorId"]

#In bar plot visitor count scale will be in millions
y=y/1000000

plt.subplot(1, 2, 1)
plt.bar(x,y,width=0.2)
plt.xlabel('Mobile vs Non-Mobile',fontdict=font)
plt.ylabel('No. of visitors in millions',fontdict=font)
plt.xticks(fontsize=14, color=tick_color)
plt.yticks(color=tick_color)
plt.title("Visitor Count",fontdict=font)
plt.legend(['visitor count'])

# subplot-2
mobile_revenue = train_df.groupby("device.isMobile")["totals.transactionRevenue"].sum().reset_index().sort_values(["totals.transactionRevenue"],ascending=False).reset_index(drop=True)
mobile_revenue['totals.transactionRevenue'] = np.log1p(mobile_revenue["totals.transactionRevenue"].values)

x = mobile_revenue["device.isMobile"]
x=[str(x[i]) for i in range(len(x))]
y = mobile_revenue["totals.transactionRevenue"]

plt.subplot(1, 2, 2)
plt.bar(x,y,width=0.2)
plt.xlabel('Mobile vs Non-Mobile',fontdict=font)
plt.ylabel('Log Transaction Revenue',fontdict=font)
plt.xticks(fontsize=14, color=tick_color)
plt.yticks(color=tick_color)
plt.title("Log Transaction Revenue",fontdict=font)
plt.legend(['Log Transaction Revenue'])
plt.show()

"""## Observations
- From the above plot vistor count of mobile is almost less than half of the non-mobile devices.
- The revenue generated through the mobile device is huge and almost more than 80% of revenue generated through non-mobile device.

## vii.) Continent Analysis
"""

#plot figure
plt.figure(figsize=(15,5))

# subplot-1
continent_count = train_df.groupby("geoNetwork.continent")["fullVisitorId"].count().reset_index().sort_values(["fullVisitorId"],ascending=False).reset_index(drop=True)

x = continent_count["geoNetwork.continent"]
y = continent_count["fullVisitorId"]

#In bar plot visitor count scale will be in millions
y=y/1000000

plt.subplot(1, 2, 1)
plt.bar(x,y,width=0.2)
plt.xlabel('continent',fontdict=font)
plt.ylabel('No. of visitors in millions',fontdict=font)
plt.xticks(fontsize=14, color=tick_color)
plt.yticks(color=tick_color)
plt.title("Visitor Count in continents",fontdict=font)
plt.legend(['visitor count'])

# subplot-2
continent_revenue = train_df.groupby("geoNetwork.continent")["totals.transactionRevenue"].sum().reset_index().sort_values(["totals.transactionRevenue"],ascending=False).reset_index(drop=True)
continent_revenue['totals.transactionRevenue'] = np.log1p(continent_revenue["totals.transactionRevenue"].values)

x = continent_revenue["geoNetwork.continent"]
y = continent_revenue["totals.transactionRevenue"]

plt.subplot(1, 2, 2)
plt.bar(x,y,width=0.2)
plt.xlabel('continent',fontdict=font)
plt.ylabel('Log Transaction Revenue',fontdict=font)
plt.xticks(fontsize=14, color=tick_color)
plt.yticks(color=tick_color)
plt.title("Log Transaction Revenue per continent",fontdict=font)
plt.legend(['Log Transaction Revenue'])
plt.show()

"""## Observations
- From the above plot we can observe that the visitor count and transaction revenue is huge in America
- From Asia and Europe even though the visitor count is almost half of vistor count in america but the revenue generated through them is around 80% of revenue generated by america.
- Even though the vistor count is less from Africa,ocenaia and others the revenue generated in those continents is huge and on par with Asia and europe.

## viii.) Traffic source Analysis
"""

#plot figure
plt.figure(figsize=(15,5))

# subplot-1
trafficSource_count = train_df.groupby("trafficSource.source")["fullVisitorId"].count().reset_index().sort_values(["fullVisitorId"],ascending=False).reset_index(drop=True)

x_temp = np.array(trafficSource_count["trafficSource.source"])
y_temp = np.array(trafficSource_count["fullVisitorId"])

#filtering Traffic source with atleast 100 count for finding huge revenue happened through which traffic source
index=np.where(y_temp>100)
x=x_temp[index]
y=y_temp[index]

#In bar plot visitor count scale will be in millions
y=y/1000000

plt.subplot(2, 1, 1)
plt.bar(x,y,width=0.2)
plt.xlabel('Source',fontdict=font)
plt.ylabel('No. of visitors in millions',fontdict=font)
plt.xticks(rotation=90, fontsize=14, color=tick_color)
plt.yticks(color=tick_color)
plt.title("Visitor Count in Source",fontdict=font)
plt.legend(['visitor count'])
plt.show()

plt.figure(figsize=(15,5))
# subplot-2
trafficSource_revenue = train_df.groupby("trafficSource.source")["totals.transactionRevenue"].sum().reset_index().sort_values(["totals.transactionRevenue"],ascending=False).reset_index(drop=True)
trafficSource_revenue['totals.transactionRevenue'] = np.log1p(trafficSource_revenue["totals.transactionRevenue"].values)

x_temp = np.array(trafficSource_revenue["trafficSource.source"])
y_temp = np.array(trafficSource_revenue["totals.transactionRevenue"])

x=x_temp[index]
y=y_temp[index]

plt.subplot(2, 1, 2)
plt.bar(x,y,width=0.2)
plt.xlabel('Source',fontdict=font)
plt.ylabel('Log Transaction Revenue',fontdict=font)
plt.xticks(rotation=90, fontsize=14, color=tick_color)
plt.yticks(color=tick_color)
plt.title("Log Transaction Revenue per continent",fontdict=font)
plt.legend(['Log Transaction Revenue'])
plt.show()

"""## Observations
- The vistor count is huge for google,yotube.com and revenue generated is also huge.
- The most important observation is even though the visitor count is very very less and almost null when compared to google and youtube sources but the revenue generated through other sources is huge and seems like the revenue generated plot is almost uniformly distributed among multiple sources.

## ix.) Trend Analysis
"""

plt.figure(figsize=(30,8))

df_date = train_df.groupby('date').count()
df_date.reset_index(inplace=True)
plt.plot_date(x=df_date['date'], y=df_date['fullVisitorId'],linestyle='solid',linewidth=5)
plt.xlabel('Date',fontdict=font)
plt.ylabel('No. of visitors',fontdict=font)
plt.xticks(fontsize=14, color=tick_color)
plt.yticks(color=tick_color)
plt.legend(['visitor count'])
plt.autoscale(True)
plt.show()

"""## Observations
- From the above plot we can observe that there is huge spike in month of december-2017 and also around 11th and 12th month of 2016.
- The transactions and reveneue generated on these periods could be very huge when compared to other days so, analytics team can invest money on these days for promotion strategies and can provide reasonable deals for customers.

# 4.) Featurization
"""

train_df.dtypes

"""## 4.1 Impute Missing Values"""

train_df["totals.transactionRevenue"].fillna(0, inplace=True)

"""## 4.2 convert numerical features to float"""

numerical_cols = ["totals.hits", "totals.pageviews", "visitNumber", "visitStartTime",'totals.timeOnSite','totals.transactions', 'totals.transactionRevenue' ]    
for col in numerical_cols:
    train_df[col].fillna(0,inplace=True)
    train_df[col] = train_df[col].astype(float)
    test_df[col].fillna(0,inplace=True)
    test_df[col] = test_df[col].astype(float)

"""## 4.3 Label Encoding for categorical features"""

categorical_columns = ["channelGrouping", 
                       "device.browser", "device.deviceCategory", "device.operatingSystem", 
                       "geoNetwork.city", "geoNetwork.continent", "geoNetwork.country", "geoNetwork.metro",
                       "geoNetwork.networkDomain", "geoNetwork.region", "geoNetwork.subContinent", 
                       "trafficSource.keyword", "trafficSource.medium",
                       "trafficSource.referralPath", "trafficSource.source",
                       "totals.sessionQualityDim" ]
           
for col in categorical_columns:
    lbl = preprocessing.LabelEncoder()
    lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))
    with open('label_encoders/'+col+'.pkl', 'wb') as file:
         pickle.dump(lbl, file)
    train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))
    test_df[col] = lbl.transform(list(test_df[col].values.astype('str')))
    print(col+' label encoding is done....')

with open('label_encoders/channelGrouping.pkl', 'rb') as file:
    lbl = pickle.load(file)

"""## 4.4 Time series featurization of train data"""

def getTrainDataFrameWithTimeFrame(df, n):

        # n=1 - 1st Timeframe
        #Train data - Aug 1st 2016 to Jan 15th 2017 (168 days)
        #Test data - Mar 2nd 2017 to May 3'rd 2017 (62 days)
        
        # n=2 - 2nd Timeframe
        #Train data - Jan 16st 2017 to Jul 2nd 2017 (168 days)
        #Test data - Aug 17th 2017 to Oct 18th 2017 (62 days)
        
        # n=3 - 3rd Timeframe
        #Train data - Jul 3rd 2017 to Dec 17th 2017 (168 days)
        #Test data - Feb 1st 2018 to Apr 4th 2018 (62 days)
        
        # n=4 - 4th Timeframe
        #Train data - Dec 18th 2017 to Jun 4th 2018 (168 days)
        #Test data - Jul 20th 2018 to Sep 20th 2018 (62 days)
        
        
    #Fetch 168 days of visitors data according to given time frame number 'n' 
    #we consider this as train time frame data
    train_tf = df.loc[(df['date'] >= min(df['date']) + timedelta(days=168*(n-1))) 
                      & (df['date'] < min(df['date']) + timedelta(days=168*n))]
    
    #Fetch 62 days of visitors data according to given time frame number 'n
    #we consider this as test time frame data
    test_tf = df.loc[(df['date'] >= max(train_tf['date']) + timedelta(days = 46))
                          & (df['date'] < max(train_tf['date']) + timedelta(days = 46 + 62))]
    
    #Fetch those customers who return to the store after cooling peiod of 46 days for timeperiod of 62 days
    customers_returned = set(train_tf["fullVisitorId"]) & set(test_tf["fullVisitorId"]) # intersection
    
    #Fetch those customers who doesn't return to the store after cooling peiod of 46 days for timeperiod of 62 days
    customers_not_returned = set(train_tf["fullVisitorId"]) - set(test_tf["fullVisitorId"]) # subtraction
    

    # Fetching customers returned data along with sum of all transaction revenue amount in test frame data
    #---------------------------------------------------------------------------------------------
    customers_returned_data = test_tf[test_tf['fullVisitorId'].isin(set(customers_returned))]  
    customers_returned_transaction_data = customers_returned_data.groupby('fullVisitorId')[['totals.transactionRevenue']]\
                                          .sum().apply(np.log1p, axis=1).reset_index()
    
    customers_returned_transaction_data.rename(columns={'totals.transactionRevenue': 'revenue'}, inplace=True)
    customers_returned_transaction_data['is_returned'] = 1
     
    
    # Fetching customers not returned data and place transaction revenue amount as 0 
    #--------------------------------------
    customers_not_returned_transaction_data = pd.DataFrame()
    customers_not_returned_transaction_data['fullVisitorId'] = list(set(customers_not_returned))
    customers_not_returned_transaction_data['is_returned'] = 0
    customers_not_returned_transaction_data['revenue'] = 0
    
    #concatenate both customers_returned_transaction_data and customers_not_returned_transaction_data
    customers_transaction_data = pd.concat([customers_returned_transaction_data, customers_not_returned_transaction_data], axis=0)\
                              .reset_index(drop=True)
    
    
    #max date and min date used for featurization purpose
    train_tf_maxdate = max(train_tf['date'])
    train_tf_mindate = min(train_tf['date'])
    
    #Additional features after time series featurization for train time frame data
    train_tf = train_tf.groupby('fullVisitorId').agg({
            'geoNetwork.networkDomain': [('networkDomain' , lambda x: x.dropna().max())], #max value of network domain
            'geoNetwork.city':          [('city' , lambda x: x.dropna().max())],  #max value of city
            'device.operatingSystem':   [('operatingSystem' , lambda x: x.dropna().max())],  #max value of Operating System
            'geoNetwork.metro':         [('metro' , lambda x: x.dropna().max())],  #max value of metro
            'geoNetwork.region':        [('region' , lambda x: x.dropna().max())],   #max vaue of region
            'channelGrouping':          [('channelGrouping' , lambda x: x.dropna().max())],  #max value of channel grouping
          'trafficSource.referralPath': [('referralPath' , lambda x: x.dropna().max())],  #max value of referral path
            'geoNetwork.country':       [('country' , lambda x: x.dropna().max())],    #max value of country
            'trafficSource.source':     [('source' , lambda x: x.dropna().max())],   #max value of source
            'trafficSource.medium':     [('medium' , lambda x: x.dropna().max())],   #max value of medium
            'trafficSource.keyword':    [('keyword', lambda x: x.dropna().max())], #max value of keyboard
            'device.browser':           [('browser' , lambda x: x.dropna().max())],  #max value of browser
            'device.deviceCategory':    [('deviceCategory', lambda x: x.dropna().max())], #max of device category
            'geoNetwork.continent':     [('continent' , lambda x: x.dropna().max())],      #max of continent value
            'geoNetwork.subContinent':  [('subcontinent' , lambda x: x.dropna().max())],  #max of sub_continent value
            'totals.timeOnSite':        [('timeOnSite_sum'  , lambda x: x.dropna().sum()),     # total timeonsite of user
                                         ('timeOnSite_min'  , lambda x: x.dropna().min()),     # min timeonsite
                                         ('timeOnSite_max'  , lambda x: x.dropna().max()),     # max timeonsite
                                         ('timeOnSite_mean' , lambda x: x.dropna().mean())],  # mean timeonsite
            'totals.pageviews':         [('pageviews_sum'  , lambda x: x.dropna().sum()),     # total of page views
                                         ('pageviews_min'  , lambda x: x.dropna().min()),     # min of page views
                                         ('pageviews_max'  , lambda x: x.dropna().max()),     # max of page views
                                         ('pageviews_mean' , lambda x: x.dropna().mean())],  # mean of page views
            'totals.hits':              [('hits_sum'  , lambda x: x.dropna().sum()),     # total of hits
                                         ('hits_min'  , lambda x: x.dropna().min()),     # min of hits
                                         ('hits_max'  , lambda x: x.dropna().max()),     # max of hits
                                         ('hits_mean' , lambda x: x.dropna().mean())],  # mean of hits
            'visitStartTime':           [('visitStartTime_counts' , lambda x: x.dropna().count())], #Count of visitStartTime
            'totals.sessionQualityDim': [('sessionQualityDim' , lambda x: x.dropna().max())], #Max value of sessionQualityDim
            'device.isMobile':          [('isMobile' ,  lambda x: x.dropna().max())], #Max value of isMobile
            'visitNumber':              [('visitNumber_max' , lambda x: x.dropna().max())],  #Maximum number of visits.
            'totals.transactions' :     [('transactions' , lambda x:x.dropna().sum())], #Summation of all the transaction counts.
            'date':                     [('days_before_the_period_start' , lambda x: x.dropna().min() - train_tf_mindate), #days_before_the_period_start for current frame.
                                         ('days_before_the_period_end', lambda x: train_tf_maxdate - x.dropna().max()), #days_before_the_period_end for current frame.
                                         ('interval_dates' , lambda x: x.dropna().max() - x.dropna().min()),  #interval calculated as the latest date on which customer visited - oldest date on which they visited.
                                         ('unqiue_date_num' , lambda x: len(set(x.dropna())))] , # Unique number of dates customer visited.           
                                                         })

    
    # Drop the parent level of features
    train_tf.columns = train_tf.columns.droplevel() 
    
    
    #merging the two dataframe train_tf having additional features and customers_transaction_data having transaction revenue data
    train_tf = pd.merge(train_tf, customers_transaction_data , left_on='fullVisitorId', right_on='fullVisitorId') 
    
    
    return train_tf

#Concatenate entire train and test data
total_data = pd.concat([train_df, test_df], axis=0).reset_index()

# Commented out IPython magic to ensure Python compatibility.
#Featurize 1st data frame
# %time train_frame1_data = getTrainDataFrameWithTimeFrame(df=total_data,n=1)
train_frame1_data.to_pickle('train_frame1')

# Commented out IPython magic to ensure Python compatibility.
#Featurize 2nd data frame
# %time train_frame2_data = getTrainDataFrameWithTimeFrame(df=total_data,n=2)
train_frame2_data.to_pickle('train_frame2')

# Commented out IPython magic to ensure Python compatibility.
#Featurize 3rd data frame
# %time train_frame3_data = getTrainDataFrameWithTimeFrame(df=total_data,n=3)
train_frame3_data.to_pickle('train_frame3')

# Commented out IPython magic to ensure Python compatibility.
#Featurize 4th data frame
# %time train_frame4_data = getTrainDataFrameWithTimeFrame(df=total_data,n=4)
train_frame4_data.to_pickle('train_frame4')

#print data sizes of each data frame
print(train_frame1_data.shape)
print(train_frame2_data.shape)
print(train_frame3_data.shape)
print(train_frame4_data.shape)

"""## 4.5 Time series featurization of test data"""

#Fetch test data i.e from May 01 2018
test_frame_data = total_data[total_data['date'] >= pd.to_datetime(20180501, infer_datetime_format=True, format="%Y%m%d")]

#Additional features after time series featurization for test time frame data
def getTestDataFrameWithTimeFrame(test_frame_data):

    test_tf_maxdate = max(test_frame_data['date'])    
    test_tf_mindate = min(test_frame_data['date']) 

    test_frame_data = test_frame_data.groupby('fullVisitorId').agg({
            'geoNetwork.networkDomain': [('networkDomain' , lambda x: x.dropna().max())], #max value of network domain
            'geoNetwork.city':          [('city' , lambda x: x.dropna().max())],  #max value of city
            'device.operatingSystem':   [('operatingSystem' , lambda x: x.dropna().max())],  #max value of Operating System
            'geoNetwork.metro':         [('metro' , lambda x: x.dropna().max())],  #max value of metro
            'geoNetwork.region':        [('region' , lambda x: x.dropna().max())],   #max vaue of region
            'channelGrouping':          [('channelGrouping' , lambda x: x.dropna().max())],  #max value of channel grouping
          'trafficSource.referralPath': [('referralPath' , lambda x: x.dropna().max())],  #max value of referral path
            'geoNetwork.country':       [('country' , lambda x: x.dropna().max())],    #max value of country
            'trafficSource.source':     [('source' , lambda x: x.dropna().max())],   #max value of source
            'trafficSource.medium':     [('medium' , lambda x: x.dropna().max())],   #max value of medium
            'trafficSource.keyword':    [('keyword', lambda x: x.dropna().max())], #max value of keyboard
            'device.browser':           [('browser' , lambda x: x.dropna().max())],  #max value of browser
            'device.deviceCategory':    [('deviceCategory', lambda x: x.dropna().max())], #max of device category
            'geoNetwork.continent':     [('continent' , lambda x: x.dropna().max())],      #max of continent value
            'geoNetwork.subContinent':  [('subcontinent' , lambda x: x.dropna().max())],  #max of sub_continent value
            'totals.timeOnSite':        [('timeOnSite_sum'  , lambda x: x.dropna().sum()),     # total timeonsite of user
                                         ('timeOnSite_min'  , lambda x: x.dropna().min()),     # min timeonsite
                                         ('timeOnSite_max'  , lambda x: x.dropna().max()),     # max timeonsite
                                         ('timeOnSite_mean' , lambda x: x.dropna().mean())],  # mean timeonsite
            'totals.pageviews':         [('pageviews_sum'  , lambda x: x.dropna().sum()),     # total of page views
                                         ('pageviews_min'  , lambda x: x.dropna().min()),     # min of page views
                                         ('pageviews_max'  , lambda x: x.dropna().max()),     # max of page views
                                         ('pageviews_mean' , lambda x: x.dropna().mean())],  # mean of page views
            'totals.hits':              [('hits_sum'  , lambda x: x.dropna().sum()),     # total of hits
                                         ('hits_min'  , lambda x: x.dropna().min()),     # min of hits
                                         ('hits_max'  , lambda x: x.dropna().max()),     # max of hits
                                         ('hits_mean' , lambda x: x.dropna().mean())],  # mean of hits
            'visitStartTime':           [('visitStartTime_counts' , lambda x: x.dropna().count())], #Count of visitStartTime
            'totals.sessionQualityDim': [('sessionQualityDim' , lambda x: x.dropna().max())], #Max value of sessionQualityDim
            'device.isMobile':          [('isMobile' ,  lambda x: x.dropna().max())], #Max value of isMobile
            'visitNumber':              [('visitNumber_max' , lambda x: x.dropna().max())],  #Maximum number of visits.
            'totals.transactions' :     [('transactions' , lambda x:x.dropna().sum())], #Summation of all the transaction counts.
            'date':                     [('days_before_the_period_start' , lambda x: x.dropna().min() - test_tf_mindate), #days_before_the_period_start for current frame.
                                         ('days_before_the_period_end', lambda x: test_tf_maxdate - x.dropna().max()), #days_before_the_period_end for current frame.
                                         ('interval_dates' , lambda x: x.dropna().max() - x.dropna().min()),  #interval calculated as the latest date on which customer visited - oldest date on which they visited.
                                         ('unqiue_date_num' , lambda x: len(set(x.dropna())))] , # Unique number of dates customer visited.           
                                                         })

    # Drop the parent level of features
    test_frame_data.columns = test_frame_data.columns.droplevel()
    
    return test_frame_data

# Commented out IPython magic to ensure Python compatibility.
# %time test_frame_data = getTestDataFrameWithTimeFrame(test_frame_data)
test_frame_data = test_frame_data.reset_index()

#add np.nan into test time frame data
test_frame_data['revenue'] = np.nan
test_frame_data['is_returned'] = np.nan

test_frame_data.to_pickle('test_frame')
print(test_frame_data.shape)

#read train and test data
train_frame1_data = pd.read_pickle("train_frame1")
train_frame2_data = pd.read_pickle("train_frame2")
train_frame3_data = pd.read_pickle("train_frame3")
train_frame4_data = pd.read_pickle("train_frame4")
test_frame_data = pd.read_pickle("test_frame")

#Concatenate all the dataframes created above to build final feature set.
final_df = pd.concat([train_frame1_data, train_frame2_data, train_frame3_data, train_frame4_data, test_frame_data], axis=0, sort=False).reset_index(drop=True)

final_df['interval_dates'] = final_df['interval_dates'].dt.days
final_df['days_before_the_period_start'] = final_df['days_before_the_period_start'].dt.days
final_df['days_before_the_period_end'] = final_df['days_before_the_period_end'].dt.days

#save final features
final_df.to_pickle("final_df")

#Read final features
final_df = pd.read_pickle("final_df")

#split train dataset from final data frame
train_df = final_df[final_df['revenue'].notnull()]

#split test dataset from final data frame
test_df = final_df[final_df['revenue'].isnull()]

target_cols = ['is_returned', 'revenue', 'fullVisitorId']

train_df[0:1].values.shape

"""# 5.) Modelling and Hyperparameter tuning on train data using     RandomSearchCV

# 5.1 Logistic regression + Linear Regression

## 5.1.1 Logistic regression classifier Hyperparameter tuning
"""

#create lr classifier
lr_classifier = SGDClassifier(loss="log", penalty="l2",class_weight='balanced')

#grid params for hyperparameter tuning
grid_Params_lr_classifier = {
                              'alpha':[0.00001,0.0001,0.01,1,10,100,1000,10000]
                            }

# Commented out IPython magic to ensure Python compatibility.
#grid search cv for hyperparameter tuning
grid_lr_classifier = GridSearchCV(lr_classifier,grid_Params_lr_classifier,cv=3,scoring='roc_auc',verbose=5,n_jobs=-1)

# %time grid_lr_classifier.fit(train_df.drop(target_cols, axis=1) , train_df['is_returned'])

#print best hypertuned parameters and best metric score
print(grid_lr_classifier.best_params_)
print(grid_lr_classifier.best_score_)

"""## 5.1.2 Linear regression classifier Hyperparameter tuning"""

#create lr regressor
lr_regressor = SGDRegressor(loss='squared_error',penalty='l2')

#grid params for hyperparameter tuning
grid_Params_lr_regressor = {
                              'alpha':[0.00001,0.0001,0.01,1,10,100,1000,10000]
                            }

# Commented out IPython magic to ensure Python compatibility.
#grid search cv for hyperparameter tuning
grid_lr_regressor = GridSearchCV(lr_regressor,grid_Params_lr_regressor, scoring = "neg_root_mean_squared_error", cv=3,verbose=5,n_jobs=-1)

# %time grid_lr_regressor.fit(train_df.drop(target_cols, axis=1)[train_df['is_returned']==1], train_df['revenue'][train_df['is_returned']==1])

#print best hypertuned parameters and best metric score
print(grid_lr_regressor.best_params_)
print(grid_lr_regressor.best_score_)

"""## 5.1.3 Run Model with Hyper tuned parameters"""

lr_final_classifier = SGDClassifier(loss="log",alpha = 100, penalty="l2",class_weight='balanced')
lr_final_regressor  = SGDRegressor(loss='squared_error',alpha=1000,penalty='l2')

final_pred = 0             

for i in range(10):
     
    lr_final_classifier.fit(train_df.drop(target_cols, axis=1) , train_df['is_returned'])
    lr_classifier_pred = lr_final_classifier.predict_proba(test_df.drop(target_cols, axis=1))[:,1]
    
    
    lr_final_regressor.fit(train_df.drop(target_cols, axis=1)[train_df['is_returned']==1], train_df['revenue'][train_df['is_returned']==1]) 
    lr_regressor_pred  = lr_final_regressor.predict(test_df.drop(target_cols, axis=1))
        
    final_pred = final_pred + (lr_classifier_pred*lr_regressor_pred)
    print("Iteration no - "+str(i+1)+" is done...")

final_pred /=10

#create data frame for visitorid and predicted log revenue
lr_pred_df = pd.DataFrame({"fullVisitorId":test_df["fullVisitorId"].values})

lr_pred_df["PredictedLogRevenue"] = final_pred

lr_pred_df.columns = ["fullVisitorId", "PredictedLogRevenue"]

#print shape of data frame
lr_pred_df.shape

#save the predicted revenue of csv file
lr_pred_df.to_csv("lr_pred.csv", index=False)

#calculate the sum of log of the revenue for each visitor from the test file
lr_test_revenue = pd.read_csv('preprocessed_test_df.csv',dtype={'fullVisitorId': 'str'},usecols=['fullVisitorId','totals.transactionRevenue']) 

lr_test_revenue = lr_test_revenue.groupby('fullVisitorId')[["totals.transactionRevenue"]].sum().apply(np.log1p, axis=1).reset_index()

#calculate the rms value 
lr_revenue_result = pd.merge(lr_pred_df, lr_test_revenue , left_on='fullVisitorId', right_on='fullVisitorId') 

lr_rms = np.sqrt(mean_squared_error(lr_revenue_result['totals.transactionRevenue'], lr_revenue_result['PredictedLogRevenue']))

print(lr_rms)

"""# 5.2 Decision tree Classification + Decision tree Regression

## 5.2.1 Decision tree Classifier Hyperparameter tuning
"""

#create dt classifier
dt_classifier = DecisionTreeClassifier()

#grid params for hyperparameter tuning
grid_Params_dt_classifier = {
              'max_depth': [2,5,7,9],
              'min_samples_split':[2,3,5,7],
              'min_samples_leaf': [1,2,3,4]
             }

# Commented out IPython magic to ensure Python compatibility.
#grid search cv for hyperparameter tuning
grid_dt_classifier = GridSearchCV(dt_classifier,grid_Params_dt_classifier,cv=3,scoring='roc_auc',verbose=5,n_jobs=-1)

# %time grid_dt_classifier.fit(train_df.drop(target_cols, axis=1) , train_df['is_returned'])

#print best hypertuned parameters and best metric score
print(grid_dt_classifier.best_params_)
print(grid_dt_classifier.best_score_)

"""## 5.2.2 Decision tree regressor Hyperparameter tuning"""

#create dt regressor
dt_regressor = DecisionTreeRegressor()

#grid params for hyperparameter tuning
grid_Params_dt_regressor = {
              'max_depth': [2,5,7,9],
              'min_samples_split':[2,3,5,7],
              'min_samples_leaf': [1,2,3,4]
             }

# Commented out IPython magic to ensure Python compatibility.
#grid search cv for hyperparameter tuning
grid_dt_regressor = GridSearchCV(dt_regressor,grid_Params_dt_regressor,cv=3,scoring='neg_root_mean_squared_error',verbose=5,n_jobs=-1)

# %time grid_dt_regressor.fit(train_df.drop(target_cols, axis=1)[train_df['is_returned']==1], train_df['revenue'][train_df['is_returned']==1])

#print best hypertuned parameters and best metric score
print(grid_dt_regressor.best_params_)
print(grid_dt_regressor.best_score_)

"""## 5.2.3 Run Model with Hyper tuned parameters"""

dt_final_classifier = DecisionTreeClassifier(max_depth=7, min_samples_leaf=1,min_samples_split=3)
dt_final_regressor  = DecisionTreeRegressor(max_depth=2, min_samples_leaf=1, min_samples_split=2)

final_pred = 0             

for i in range(10):
    
    dt_final_classifier.fit(train_df.drop(target_cols, axis=1) , train_df['is_returned'])
    dt_classifier_pred = dt_final_classifier.predict_proba(test_df.drop(target_cols, axis=1))[:,1]
    
    dt_final_regressor.fit(train_df.drop(target_cols, axis=1)[train_df['is_returned']==1], train_df['revenue'][train_df['is_returned']==1])
    dt_regressor_pred  = dt_final_regressor.predict(test_df.drop(target_cols, axis=1))
    
    final_pred = final_pred + (dt_classifier_pred*dt_regressor_pred)
    print("Iteration no - "+str(i+1)+" is done...")

final_pred /=10

#create data frame for visitorid and predicted log revenue
dt_pred_df = pd.DataFrame({"fullVisitorId":test_df["fullVisitorId"].values})

dt_pred_df["PredictedLogRevenue"] = final_pred

dt_pred_df.columns = ["fullVisitorId", "PredictedLogRevenue"]

#print shape of data frame
dt_pred_df.shape

#save the predicted revenue of csv file
dt_pred_df.to_csv("dt_pred.csv", index=False)

#calculate the sum of log of the revenue for each visitor from the test file
dt_test_revenue = pd.read_csv('preprocessed_test_df.csv',dtype={'fullVisitorId': 'str'},usecols=['fullVisitorId','totals.transactionRevenue']) 

dt_test_revenue = dt_test_revenue.groupby('fullVisitorId')[["totals.transactionRevenue"]].sum().apply(np.log1p, axis=1).reset_index()

#calculate the rms value 
dt_revenue_result = pd.merge(dt_pred_df, dt_test_revenue , left_on='fullVisitorId', right_on='fullVisitorId') 

dt_rms = np.sqrt(mean_squared_error(dt_revenue_result['totals.transactionRevenue'], dt_revenue_result['PredictedLogRevenue']))

print(dt_rms)

"""# 5.3 Random forest + Random regressor

## 5.3.1 Random forest Classifier Hyperparameter tuning
"""

#create rf classifier
rf_classifier = RandomForestClassifier()

#grid params for hyperparameter tuning
grid_Params_rf_classifier = {
              'n_estimators':[100,300,500,700],
              'max_depth': [2,5,7,9],
              'min_samples_split':[2,3,5,7],
              'min_samples_leaf': [1,2,3,4],
              'bootstrap': [True, False]
             }

# Commented out IPython magic to ensure Python compatibility.
#random search cv for hyperparameter tuning
grid_rf_classifier = RandomizedSearchCV(rf_classifier,grid_Params_rf_classifier,cv=3,scoring='neg_log_loss',n_jobs=-1,verbose=5)

# %time grid_rf_classifier.fit(train_df.drop(target_cols, axis=1) , train_df['is_returned'])

#print best hypertuned parameters and best metric score
print(grid_rf_classifier.best_params_)
print(grid_rf_classifier.best_score_)

"""## 5.3.2 Random forest Regressor Hyperparameter tuning"""

#create rf regressor
rf_regressor = RandomForestRegressor()

#grid params for hyperparameter tuning
grid_Params_rf_regressor = {
              'n_estimators':[100,300,500,700],
              'max_depth': [2,5,7,9],
              'min_samples_split':[2,3,5,7],
              'min_samples_leaf': [1,2,3,4],
              'bootstrap': [True, False]
             }

# Commented out IPython magic to ensure Python compatibility.
#random search cv for hyperparameter tuning
grid_rf_regressor = RandomizedSearchCV(rf_regressor,grid_Params_rf_regressor,cv=3,scoring='neg_root_mean_squared_error',n_jobs=-1,verbose=5)

# %time grid_rf_regressor.fit(train_df.drop(target_cols, axis=1)[train_df['is_returned']==1], train_df['revenue'][train_df['is_returned']==1])

#print best hypertuned parameters and best metric score
print(grid_rf_regressor.best_params_)
print(grid_rf_regressor.best_score_)

"""## 5.3.3 Run Model with Hyper tuned parameters"""

rf_final_classifier = RandomForestClassifier(n_estimators=500,min_samples_split=3,min_samples_leaf=3,max_depth= 9,
                                             bootstrap = False,n_jobs=-1)
rf_final_regressor  = RandomForestRegressor(n_estimators= 500,min_samples_split=5,min_samples_leaf=3,max_depth=7,
                                            bootstrap=True,n_jobs=-1)

final_pred = 0             

for i in range(10):
    
    rf_final_classifier.fit(train_df.drop(target_cols, axis=1) , train_df['is_returned'])
    rf_classifier_pred = rf_final_classifier.predict_proba(test_df.drop(target_cols, axis=1))[:,1]
    
    rf_final_regressor.fit(train_df.drop(target_cols, axis=1)[train_df['is_returned']==1], train_df['revenue'][train_df['is_returned']==1])
    rf_regressor_pred  = rf_final_regressor.predict(test_df.drop(target_cols, axis=1))
    
    final_pred = final_pred + (rf_classifier_pred*rf_regressor_pred)
    print("Iteration no - "+str(i+1)+" is done...")

final_pred /=10

#create data frame for visitorid and predicted log revenue
rf_pred_df = pd.DataFrame({"fullVisitorId":test_df["fullVisitorId"].values})

rf_pred_df["PredictedLogRevenue"] = final_pred

rf_pred_df.columns = ["fullVisitorId", "PredictedLogRevenue"]

#print shape of data frame
rf_pred_df.shape

#save the predicted revenue of csv file
rf_pred_df.to_csv("rf_pred.csv", index=False)

#calculate the sum of log of the revenue for each visitor from the test file
rf_test_revenue = pd.read_csv('preprocessed_test_df.csv',dtype={'fullVisitorId': 'str'},usecols=['fullVisitorId','totals.transactionRevenue']) 

rf_test_revenue = rf_test_revenue.groupby('fullVisitorId')[["totals.transactionRevenue"]].sum().apply(np.log1p, axis=1).reset_index()

#calculate the rms value 
rf_revenue_result = pd.merge(rf_pred_df, rf_test_revenue , left_on='fullVisitorId', right_on='fullVisitorId') 

rf_rms = np.sqrt(mean_squared_error(rf_revenue_result['totals.transactionRevenue'], rf_revenue_result['PredictedLogRevenue']))

print(rf_rms)

"""# 5.4 LightGBM classification + LightGBM regression

## 5.4.1 LightGBM Classifier Hyperparameter tuning
"""

#create lgb classifier
lgbm_classifier = lgb.LGBMClassifier()

#grid params for hyperparameter tuning
grid_Params_lgbm_classifier = {
    'learning_rate': [0.002,0.01,0.015],    #Learning rate
    'n_estimators': [50,150,200],           #number of boosting iterations  
    'num_leaves': [6,9,12,15,18],           #number of leaves in full tree
    'boosting_type' : ['gbdt'],
    'objective' : ['binary'],               #Binary Classification model to predict whether customer will return during test window
    'metric' : ['binary_logloss'],          #Performance metric as "Binary Logloss"  
    'colsample_bytree' : [0.6, 0.8, 1],     #LightGBM will select 80% of features before training each tree
    'subsample' : [0.7,0.9, 1],             #this will randomly select part of data without resampling
    'reg_alpha' : [0,1],                    #L1 regularization
    'reg_lambda' : [0,1],                   #L2 regularization
    'max_leaves': [128,256,512],            #Maximum number of nodes to be added.   
    'min_child_samples' : [10,20]            #Minimum number of data points needed in a child (leaf) node.
     }

# Commented out IPython magic to ensure Python compatibility.
#random search cv for hyperparameter tuning
grid_lgbm_classifier = RandomizedSearchCV(lgbm_classifier, grid_Params_lgbm_classifier,cv=3,verbose=0)

# %time grid_lgbm_classifier.fit(train_df.drop(target_cols, axis=1),train_df['is_returned'])

# Print the best parameters found
print(grid_lgbm_classifier.best_params_)
print(grid_lgbm_classifier.best_score_)

"""## 5.4.2 LightGBM Regressor Hyperparameter tuning"""

lgbm_regressor = lgb.LGBMRegressor()

grid_Params_lgbm_regressor = {
    'learning_rate': [0.002,0.01,0.015],    #Learning rate
    'n_estimators': [50,150,200],           #number of boosting iterations  
    'num_leaves': [6,9,12,15,18],           #number of leaves in full tree
    'boosting_type' : ['gbdt'],
    'objective' : ['regression'],          #Regression model to predict transaction amount
    'metric' : ['rmse'],                    #Performance metric as "RMSE  
    'colsample_bytree' : [0.6, 0.8, 1],     #LightGBM will select 80% of features before training each tree
    'subsample' : [0.7,0.9, 1],             #this will randomly select part of data without resampling
    'reg_alpha' : [0,1],                    #L1 regularization
    'reg_lambda' : [0,1],                   #L2 regularization
    'max_leaves': [128,256,512],            #Maximum number of nodes to be added.   
    'min_child_samples' : [10,20]            #Minimum number of data points needed in a child (leaf) node.
     }

# Commented out IPython magic to ensure Python compatibility.
grid_lgbm_regressor = RandomizedSearchCV(lgbm_regressor, grid_Params_lgbm_regressor,cv=3)

# %time grid_lgbm_regressor.fit(train_df.drop(target_cols, axis=1)[train_df['is_returned']==1], train_df['revenue'][train_df['is_returned']==1])

#print best hypertuned parameters and best metric score
print(grid_lgbm_regressor.best_params_)
print(grid_lgbm_regressor.best_score_)

"""## 5.4.3 Run Model with Hyper tuned parameters"""

grid_params_lgb_classifier = {
        "objective" : "binary",                #Binary Classification model to predict whether customer will return during test window
        "metric" : "binary_logloss",           #Performance metric as "Binary Logloss"  
        "max_leaves": 512,                     #Maximum number of nodes to be added. 
        "num_leaves" : 12,                     #number of leaves in full tree    
        "min_child_samples" : 20,              #Minimum number of data points needed in a child (leaf) node.  
        "learning_rate" : 0.02,                #Learning rate
        "subsample" : 0.9,                     #this will randomly select part of data without resampling
        "colsample_bytree" : 1,              #LightGBM will select 80% of features before training each tree
        "bagging_frequency" : 1,               #Perform bagging at every k iteration
        "n_estimators" : 200,                  #number of boosting iterations
        "reg_alpha" : 0,                       #L1 regularization
        "reg_lambda": 1,                       #L2 regularization
        "boosting_type" : "gbdt"}

grid_params_lgb_regressor = {
        "objective" : "regression",                 #Regression model to predict transaction amount
        "metric" : "rmse",                          #Performance metric as "RMSE"
        "max_leaves": 512,                          #Maximum number of nodes to be added. 
        "num_leaves" : 18,                           #number of leaves in full tree
        "min_child_samples" : 10,                    #Minimum number of data points needed in a child (leaf) node.   
        "learning_rate" : 0.015,                     #Learning rate
        "subsample" : 1,                            #this will randomly select part of data without resampling
        "colsample_bytree" : 0.6,                   #LightGBM will select 80% of features before training each tree
        "bagging_frequency" : 1,                    #Perform bagging at every k iteration
        "n_estimators" : 200,                       #number of boosting iterations
        "reg_alpha" : 1,                            #L1 regularization
        "reg_lambda": 1,                            #L2 regularization
        "boosting_type" : "gbdt"}

#create datasets for customers returned and customers revenue for those who returned
train_returned = lgb.Dataset(train_df.drop(target_cols, axis=1), label = train_df['is_returned'])

train_revenue = lgb.Dataset(train_df.drop(target_cols, axis=1)[train_df['is_returned']==1], 
                         label=train_df['revenue'][train_df['is_returned']==1])

final_lgb_sum = 0    


for i in range(10):     
    
    lgb_final_classifier = lgb.train(grid_params_lgb_classifier, train_returned)
    lgb_classifier_pred     = lgb_final_classifier.predict(test_df.drop(target_cols, axis=1))
    
 
    lgb_final_regressor = lgb.train(grid_params_lgb_regressor, train_revenue)
    lgb_regressor_pred = lgb_final_regressor.predict(test_df.drop(target_cols, axis=1))
    

    final_lgb_sum = final_lgb_sum + lgb_classifier_pred*lgb_regressor_pred
    
    print("Iteration no - "+str(i+1)+" is done...")

final_pred = final_lgb_sum/10

#create data frame for visitorid and predicted log revenue
lgb_pred_df = pd.DataFrame({"fullVisitorId":test_df["fullVisitorId"].values})

lgb_pred_df["PredictedLogRevenue"] = final_pred

lgb_pred_df.columns = ["fullVisitorId", "PredictedLogRevenue"]

#print shape of data frame
lgb_pred_df.shape

#save the predicted revenue of csv file
lgb_pred_df.to_csv("lgb_pred.csv", index=False)

#calculate the sum of log of the revenue for each visitor from the test file
lgb_test_revenue = pd.read_csv('preprocessed_test_df.csv',dtype={'fullVisitorId': 'str'},usecols=['fullVisitorId','totals.transactionRevenue']) 

lgb_test_revenue = lgb_test_revenue.groupby('fullVisitorId')[["totals.transactionRevenue"]].sum().apply(np.log1p, axis=1).reset_index()

#calculate the rms value 
lgb_revenue_result = pd.merge(lgb_pred_df, lgb_test_revenue , left_on='fullVisitorId', right_on='fullVisitorId') 

lgb_rms = np.sqrt(mean_squared_error(lgb_revenue_result['totals.transactionRevenue'], lgb_revenue_result['PredictedLogRevenue']))

print(lgb_rms)

"""# 5.5 XGB classification + XGB regression

## 5.5.1 XGB Classifier Hyperparameter tuning
"""

#create lgb classifier
xgb_classifier = xgb.XGBClassifier()

#grid params for hyperparameter tuning
grid_Params_xgb_classifier = {
    'learning_rate': [0.002,0.01,0.015],    #Learning rate
    'n_estimators': [50,150,200],           #number of boosting iterations  
    'objective' : ['binary:logistic'],      #Binary Classification model to predict whether customer will return during test window  
    'colsample_bytree' : [0.6, 0.8, 1],     #LightGBM will select 80% of features before training each tree
    'subsample' : [0.7,0.9, 1],             #this will randomly select part of data without resampling
    'alpha' : [0,1],                        #L1 regularization
    'lambda' : [0,1],                       #L2 regularization
    'gamma': [0.5, 1, 1.5, 2],
    'max_leaves': [128,256,512],            #Maximum number of nodes to be added.   
    'max_depth': [3, 5, 8]
     }

# Commented out IPython magic to ensure Python compatibility.
#random search cv for hyperparameter tuning
grid_xgb_classifier = RandomizedSearchCV(xgb_classifier, grid_Params_xgb_classifier,cv=3,scoring='roc_auc',n_jobs=-1,verbose=5)

# %time grid_xgb_classifier.fit(train_df.drop(target_cols, axis=1),train_df['is_returned'])

# Print the best parameters found
print(grid_xgb_classifier.best_params_)
print(grid_xgb_classifier.best_score_)

"""## 5.5.2 XGB Regressor Hyperparameter tuning"""

xgb_regressor = xgb.XGBRegressor(verbosity=1)

#grid params for hyperparameter tuning
grid_Params_xgb_regressor = {
    'learning_rate': [0.002,0.01,0.015],    #Learning rate
    'n_estimators': [50,150,200],           #number of boosting iterations                 
    'colsample_bytree' : [0.6, 0.8, 1],     #LightGBM will select 80% of features before training each tree
    'subsample' : [0.7,0.9, 1],             #this will randomly select part of data without resampling
    'alpha' : [0,1],                    #L1 regularization
    'lambda' : [0,1],                   #L2 regularization
    'gamma': [0.5, 1, 1.5, 2],
    'max_leaves': [128,256,512],            #Maximum number of nodes to be added.   
    'max_depth': [3, 5, 8]
     }

# Commented out IPython magic to ensure Python compatibility.
grid_xgb_regressor = RandomizedSearchCV(xgb_regressor, grid_Params_xgb_regressor,cv=3,scoring='neg_root_mean_squared_error',n_jobs=-1,verbose=5)

# %time grid_xgb_regressor.fit(train_df.drop(target_cols, axis=1)[train_df['is_returned']==1], train_df['revenue'][train_df['is_returned']==1])

# Print the best parameters found
print(grid_xgb_regressor.best_params_)
print(grid_xgb_regressor.best_score_)

"""## 5.5.3 Run Model with Hyper tuned parameters"""

grid_Params_xgb_classifier = {
                               'subsample': 0.9,
                               'n_estimators': 200,
                                'max_leaves': 256, 
                               'max_depth': 8, 
                               'learning_rate': 0.002, 
                               'lambda': 0, 
                               'gamma': 1.5, 
                               'colsample_bytree': 0.6,
                               'alpha': 0
                            }
grid_Params_xgb_regressor = {
                             'subsample': 1, 
                             'n_estimators': 150, 
                             'max_leaves': 512,
                             'max_depth': 3, 
                             'learning_rate': 0.015, 
                             'lambda': 0,
                             'gamma': 2, 
                             'colsample_bytree': 0.6,
                             'alpha': 1
                            }

#create datasets for customers returned and customers revenue for those who returned
train_returned = xgb.DMatrix(train_df.drop(target_cols, axis=1).values, label = train_df['is_returned'].values)

train_revenue = xgb.DMatrix(train_df.drop(target_cols, axis=1)[train_df['is_returned']==1].values, 
                         label=train_df['revenue'][train_df['is_returned']==1].values)

final_xgb_sum = 0    


for i in range(10):     
    
    xgb_final_classifier = xgb.train(grid_Params_xgb_classifier, train_returned)
    xgb_classifier_pred  = xgb_final_classifier.predict(xgb.DMatrix(test_df.drop(target_cols, axis=1).values))
    
 
    xgb_final_regressor = xgb.train(grid_Params_xgb_regressor, train_revenue)
    xgb_regressor_pred = xgb_final_regressor.predict(xgb.DMatrix(test_df.drop(target_cols, axis=1).values))
    

    final_xgb_sum = final_xgb_sum + xgb_classifier_pred*xgb_regressor_pred
    
    print("Iteration no - "+str(i+1)+" is done...")

final_pred = final_xgb_sum/10

#create data frame for visitorid and predicted log revenue
xgb_pred_df = pd.DataFrame({"fullVisitorId":test_df["fullVisitorId"].values})

xgb_pred_df["PredictedLogRevenue"] = final_pred

xgb_pred_df.columns = ["fullVisitorId", "PredictedLogRevenue"]

#print shape of data frame
xgb_pred_df.shape

#save the predicted revenue of csv file
xgb_pred_df.to_csv("xgb_pred.csv", index=False)

#calculate the sum of log of the revenue for each visitor from the test file
xgb_test_revenue = pd.read_csv('preprocessed_test_df.csv',dtype={'fullVisitorId': 'str'},usecols=['fullVisitorId','totals.transactionRevenue']) 

xgb_test_revenue = xgb_test_revenue.groupby('fullVisitorId')[["totals.transactionRevenue"]].sum().apply(np.log1p, axis=1).reset_index()

#calculate the rms value 
xgb_revenue_result = pd.merge(xgb_pred_df, xgb_test_revenue , left_on='fullVisitorId', right_on='fullVisitorId') 

xgb_rms = np.sqrt(mean_squared_error(xgb_revenue_result['totals.transactionRevenue'], xgb_revenue_result['PredictedLogRevenue']))

print(xgb_rms)

"""# 5.6 Deep learning"""

import tensorflow as tf
from tensorflow import keras
from keras import optimizers
from keras.models import Sequential,load_model
from keras.layers import Dense
from keras.callbacks import ModelCheckpoint

"""## 5.6.1 Deep learing classification model"""

dl_classification = Sequential()
dl_classification.add(Dense(256, kernel_initializer='he_normal', activation='relu',input_dim=36))
dl_classification.add(Dense(128, kernel_initializer='he_normal', activation='relu'))
dl_classification.add(Dense(32, kernel_initializer='he_normal', activation='relu'))
dl_classification.add(Dense(1,activation='sigmoid'))

adam = optimizers.adam(lr=0.03)
dl_classification.compile(loss='binary_crossentropy', optimizer=adam)

dl_classification.summary()

#create file path 
filepath="classification_loss/weights-{epoch:02d}-{loss:.4f}.hdf5"

#create ModelCheckpoint object
checkpoint = ModelCheckpoint(filepath=filepath, monitor='loss',  verbose=1, save_best_only=True, mode='min')

dl_classification.fit(x = train_df.drop(target_cols, axis=1) ,y=train_df['is_returned'],
                    batch_size=100, epochs=10, callbacks = [checkpoint],
                    verbose=1)

"""## 5.6.2 Deep learing regression model"""

dl_regression = Sequential()
dl_regression.add(Dense(256, kernel_initializer='he_normal', activation='relu',input_dim=36))
dl_regression.add(Dense(128, kernel_initializer='he_normal', activation='relu'))
dl_regression.add(Dense(32, kernel_initializer='he_normal', activation='relu'))
dl_regression.add(Dense(1,activation='sigmoid'))

adam = optimizers.adam(lr=0.03)
dl_regression.compile(loss='mse', optimizer=adam)

dl_regression.summary()

#create file path 
filepath="regression_loss/weights-{epoch:02d}-{loss:.4f}.hdf5"

#create ModelCheckpoint object
checkpoint = ModelCheckpoint(filepath=filepath, monitor='loss',  verbose=1, save_best_only=True, mode='min')

dl_regression.fit(x=train_df.drop(target_cols, axis=1)[train_df['is_returned']==1], y= train_df['revenue'][train_df['is_returned']==1],
                    batch_size=100, epochs=100, callbacks=[checkpoint], 
                    verbose=1)

#load best classification and regression models
dl_classification = load_model('classification_loss/weights-02-0.0378.hdf5')
dl_regression = load_model('regression_loss/weights-71-18.8996.hdf5')

#predict and evaluate the revenue
classification_pred = dl_classification.predict(test_df.drop(target_cols, axis=1))
regression_pred = dl_regression.predict(test_df.drop(target_cols, axis=1))
regression_pred[regression_pred<0] = 0
revenue_pred = classification_pred*regression_pred

#create data frame for visitorid and predicted log revenue
dl_pred_df = pd.DataFrame({"fullVisitorId":test_df["fullVisitorId"].values})

dl_pred_df["PredictedLogRevenue"] = revenue_pred

dl_pred_df.columns = ["fullVisitorId", "PredictedLogRevenue"]

#save the predicted revenue of csv file
dl_pred_df.to_csv("dl_pred_df.csv", index=False)

#calculate the sum of log of the revenue for each visitor from the test file
dl_test_revenue = pd.read_csv('preprocessed_test_df.csv',dtype={'fullVisitorId': 'str'},usecols=['fullVisitorId','totals.transactionRevenue']) 

dl_test_revenue = dl_test_revenue.groupby('fullVisitorId')[["totals.transactionRevenue"]].sum().apply(np.log1p, axis=1).reset_index()

#calculate the rms value 
dl_revenue_result = pd.merge(dl_pred_df, dl_test_revenue , left_on='fullVisitorId', right_on='fullVisitorId') 

dl_rms = np.sqrt(mean_squared_error(dl_revenue_result['totals.transactionRevenue'], dl_revenue_result['PredictedLogRevenue']))

print(dl_rms)

"""# 6.) Conclusions"""

x = PrettyTable()

x.field_names = ["Model.no","Model", "RMSE(our calculation)","kaggle private score"]

x.add_row([" "," "," "," "])
x.add_row(["Model-1:"," "," "," "])
x.add_row(["","Decision tree Classification (with auc = 0.8520)","",""])
x.add_row(["","Decision tree Regression     (with rmse = -4.1331)","",""])
x.add_row(["","Final model","2.0928","0.88201(Best)"])

x.add_row([" "," "," "," "])
x.add_row(["Model-2:"," "," "," "])
x.add_row(["","Random-forest Classification (with logloss = -0.029)","",""])
x.add_row(["","Random-forest Regression     (with rmse =-4.0368)","",""])
x.add_row(["","Final model","2.0969","0.88204"])

x.add_row([" "," "," "," "])
x.add_row(["Model-3:"," "," "," "])
x.add_row(["","LightGBM Classification (with logloss =0.993)","",""])
x.add_row(["","LightGBM Regression     (with rmse =0.0775)","",""])
x.add_row(["","Final model","2.0980","0.88208"])

x.add_row([" "," "," "," "])
x.add_row(["Model-4:"," "," "," "])
x.add_row(["","Deep learning Classification (with loss=0.0378)","",""])
x.add_row(["","Deep learning Regression     (with mse = 18.8996)","",""])
x.add_row(["","Final model","2.1138","0.88800"])


x.add_row([" "," "," "," "])
x.add_row(["Model-5:"," "," "," "])
x.add_row(["","XGB Classification (with auc = 0.875)","",""])
x.add_row(["","XGB Regression  (with rmse = -4.025)","",""])
x.add_row(["","Final model","2.0799","0.91339"])


x.add_row([" "," "," "," "])
x.add_row(["Model-6:"," "," "," "])
x.add_row(["","Logistic regression Classification (with auc = 0.8241)","",""])
x.add_row(["","Linear Regression     (with rmse = -3.29*10^16)","",""])
x.add_row(["","Final model","1.247*10^16","1.247*10^16"])

print(x)

from IPython.display import Image
Image(filename="Best_kaggle_private_score.png")

"""# 7.) Summary

- Understanding basic 80/20 rule i.e most of the revenue will be generated by very few customers
- ML formulation of the business problem using Hurdle model which is combination of both classification and regression
- preprocessing huge train(25.5 GB) and test(7.62GB) datasets by dividing into smaller chunks
- Preprocessing json columns and normalizing them to normal tables.
- Performimg data cleaning by Removing redundant columns by finding out constant columns
- Exploratory data analysis for all the impacting features for generating revenue and writing observations
- The most important idea is time series featurizarion i.e dividing train datasets and test datsets and finding customers who returned after a cooling period and forming a dataframe suitable for classification problem by introducing a 'is_retuned' class label and for regression problem summing the transaction revenue and introducing a 'revenue' class label
- Adding additional features using operations like Max,Min,Count,Mean etc. with respect to that feature.
- Hyperparameter tuning performed on classification and regression models below
    - Logistic regression and Linear regression
    - Decision tree classification and Decision tree regression
    - Random forest classification and Random forest regression
    - LightGBM classification and Light GBM regression
    - XGB classification and XGB regression
    - Deep learning classification and regression
- Decision tree model outperformed compared to powerful models like random forest and LightGBM and gave kaggle leaderboard private score of 0.88201
- So, Decision tree model is best model and it outperformed compared to other models
- Got 2nd rank(top 1 %) on Kaggle private score leaderboard
"""